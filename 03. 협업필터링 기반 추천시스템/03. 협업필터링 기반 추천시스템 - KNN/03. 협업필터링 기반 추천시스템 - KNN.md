이번 포스팅부터 추천시스템의 입문자분들을 위한 추천시스템 글을 작성해보도록 하겠습니다. 해당 글은 T-아카데미에서 발표한 추천시스템 - 입문하기의 자료에 딥러닝을 이용한 추천시스템과 추천시스템 대회를 분석한 내용을 추가한 글입니다. 해당 자료보다 더욱더 좋은 자료들이 [페이스북 그룹 Recommender System KR](https://www.facebook.com/groups/2611614312273351)에 있으니 많은 관심 부탁합니다. 

- [[01. 추천시스템 이해]](https://eda-ai-lab.tistory.com/522)
- [02. 컨텐츠 기반 추천시스템]
  - [[02. 유사도 함수 및 평가 함수]](https://eda-ai-lab.tistory.com/523)
  - [[02. TF-IDF를 이용한 추천시스템]](https://eda-ai-lab.tistory.com/524)
  - [[02. Word2Vec을 이용한 추천시스템]](https://eda-ai-lab.tistory.com/525)
- [03. 협업필터링 기반 추천시스템]
  - **[[03. KNN을 이용한 추천시스템]](https://eda-ai-lab.tistory.com/526)**
  - [[03. SGD을 이용한 추천시스템]](https://eda-ai-lab.tistory.com/527)
  - [[03. ALS을 이용한 추천시스템]](https://eda-ai-lab.tistory.com/528)
- [04. 딥러닝을 이용한 추천시스템]
  - [[04. Deep Neural Networks for YouTube Recommendations]](https://eda-ai-lab.tistory.com/529)
  - [[04. Training Deep AutoEncoders for Collaborative Filtering]](https://eda-ai-lab.tistory.com/530)
  - [[04. Wide & Deep Learning for Recommender Systems]](https://eda-ai-lab.tistory.com/531)
  - [[04. Factorization Machine]](https://eda-ai-lab.tistory.com/532)
- [05. 추천시스템 실습하기]
- [06. 추천시스템 대회 분석하기]
  - [06. 카카오 아레나 2차 대회]
  - [06. 카카오 아레나 3차 대회]
  - [06. RecSys2020 대회]

# 협업필터링 기반 추천시스템 - KNN 

협업필터링이란? 
<br> 
협업필터링은 사용자의 구매 패턴이나 평점을 가지고 다른 사람들의 구매 패턴, 평점을 통해서 추천을 하는 방법입니다. 추가적인 사용자의 개인정보나 아이템의 정보가 없이도 추천할 수 있는게 큰 장점이며 2006부터 2009년동안 열린 Netflix Prize Competition에서 우승한 알고리즘으로 유명새를 떨쳤습니다. 이러한 협업필터링에서도 Neighborhood based Collaborative Filtering은 메모리 기반 알고리즘으로 협업 필터링을 위해 개발된 초기 알고리즘입니다. (**메모리 기반 알고리즘**(Neighborhood model 기준)은 유저와 아이템에 대한 matrix를 만든 뒤, 유저 **기반** 혹은 아이템 **기반**으로 유사한 객체를 찾은 뒤 빈공간을 추론하는 **알고리즘**이다.) 이러한 협업필터링은 아래의 유저 기반 혹은 아이템 기반으로 나뉩니다. 

1. User-based collaborative filtering : 사용자의 구매 패턴(평점)과 유사한 사용자를 찾아서 추천 리스트 생성
2. Item-based collaborative filtering : 특정 사용자가 준 점수간의 유사한 상품을 찾아서 추천 리스트 생성

<br>

![](https://drive.google.com/uc?export=view&id=1JfDSRwExlGUL_XWrd0Td6J3sLz36emA7)

</br> 

<center>이미지 출처 : 공대인들이 직접쓰는 컴퓨터공부방</center>
그 중에서도 K Nearest Neighbors는 가장 유사한 K 명의 Neighbors를 통해서 예측하는 방법입니다. 

<br> 

![](https://drive.google.com/uc?export=view&id=1LjFlGPZzIMaLD7C6OQo9n5raD12ez6HZ)

</br> 

예를 들어, 아래와 같이 유저가 자신의 선호도를 직접  표현한 데이터가 있다고 생각하겠습니다. 이렇게, 유저가 평점등을 표현한 데이터를 Explicit Feedback 된 데이터라고 합니다. 

|         | **아이템1** | **아이템2** | **아이템3** | **아이템4** | **아이템5** | **아이템6** |
| ------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| 사용자1 | 7           | 6           | 7           | 4           | 5           | 4           |
| 사용자2 | 6           | 7           | **?**       | 4           | 3           | 4           |
| 사용자3 | **?**       | 3           | 3           | 1           | 1           | **?**       |
| 사용자4 | 1           | 2           | 2           | 3           | 3           | 4           |
| 사용자5 | 1           | ?           | 1           | 2           | 3           | 3           |

이제 이러한 사용자-아이템의 평점 행렬에서 비슷한 유저를 유사도행렬을 통해서 찾습니다. 보통은 코사인 유사도를 많이 사용하지만 피어슨 유사도도 많이 사용합니다. 한번, 사용자 3의 물음표를 채우기 위해서 사용자 1부터 5까지의 유사도를 계산해보도록 하겠습니다. 물음표는 제외하고 값을 가지고 있는 부분만을 통해서 유사도를 계산하게 됩니다. 

<br> 

![](https://drive.google.com/uc?export=view&id=10uslhTS6zgRD1dHIxLsnxL49SDcV3qoe)

</br> 

|         | 아이템1 | 아이템2 | 아이템3 | 아이템4 | 아이템5 | 아이템6 | 평균 | Cosine(i, 3) | Pearson(i, 3) |
| :------ | ------- | ------- | ------- | ------- | ------- | ------- | ---- | ------------ | ------------- |
| 사용자1 | 7       | 6       | 7       | 4       | 5       | 4       | 5.5  | 0.956        | 0.894         |
| 사용자2 | 6       | 7       | ?       | 4       | 3       | 4       | 4.8  | 0.981        | 0.939         |
| 사용자3 | ?       | 3       | 3       | 1       | 1       | ?       | 2    | 1.0          | 1.0           |
| 사용자4 | 1       | 2       | 2       | 3       | 3       | 4       | 2.5  | 0.789        | -1.0          |
| 사용자5 | 1       | ?       | 1       | 2       | 3       | 3       | 2    | 0.645        | -0.817        |

이렇게 해서 계산한 코사인 유사도와 피어슨 유사도는 위와 같습니다. 유사도를 통해서 사용자1과 사용자2와 유사하다는 것을 알았습니다. 이를 통해서, 두 명의 사용자의 평점으로 사용자3의 ?를 채우게 됩니다. 보통 계산은 평점 * 유사도값의 가중평균을 통해서 계산합니다. 

<br> 

![](https://drive.google.com/uc?export=view&id=1SIadzsU-SnGvNnM-3bEMazUEsuW8MgeF)

</br> 

하지만, 이렇게 계산한 값에는 "편향"이라는게 존재합니다. 그 이유는 사용자의 평균에 있는데, 사용자1과 사용자2는 원래 다른 사용자들에 비해서 평균 평점이 높은 사용자들이었습니다. 이러한 편향을 막기 위해 수정된 평점을 사용하는데, 사용자의 평점과 아이템의 평균 평점을 반영해서 위와 같이 수정된 평점을 사용하게 됩니다. 계산은 위의 이미지와 같이 계산하게 됩니다. 
<br> 
위와 같은 방식이 비슷한 사용자들 통해서 ?를 채우는 방법이었다면, 아래는 비슷한 아이템을 통해서 ?를 채우는 Item Based Collaborative Filtering입니다. 

|         | 아이템1 | 아이템2 | 아이템3 | 아이템4 | 아이템5 | 아이템6 | 평균 |
| ------- | ------- | ------- | ------- | ------- | ------- | ------- | ---- |
| 사용자1 | 7       | 6       | 7       | 4       | 5       | 4       | 5.5  |
| 사용자2 | 6       | 7       | ?       | 4       | 3       | 4       | 4.8  |
| 사용자3 | ?       | 3       | 3       | 1       | 1       | ?       | 2    |
| 사용자4 | 1       | 2       | 2       | 3       | 3       | 4       | 2.5  |
| 사용자5 | 1       | ?       | 1       | 2       | 3       | 3       | 2    |

User Based Collaborative Filtering와 같이 편향을 제거해주는 과정이 필요합니다. 이때에도 위와 동일하게 사용자의 평균 평점을 아이템의 평점에 빼줌으로서 계산하게 됩니다. 이후, 코사인 유사도를 계산하면 아이템간의 유사도를 계산할 수 있습니다. 

<br> 

![](https://drive.google.com/uc?export=view&id=1VNM4WIpZMZ4lmLKRo4T2za6J7I0gjwGI)

</br> 

이제 유사한 아이템을 찾아보도록 하겠습니다. 아이템1의 경우 아이템2와 3과 유사하다는 것을 알 수 있습니다. 그래서, 둘의 가중평균을 통해서 사용자3의 아이템1과 아이템6의 유사도를 계산할 수 있습니다. 

<br> 

![](https://drive.google.com/uc?export=view&id=1sFYZNgNLv7fHrW3WszgSS9_fO-mFY0rY)

</br> 

이런 Neighborhood based method의 장점과 단점은 아래와 같습니다. 

[장점]

- 간단하고 직관적인 접근 방식 때문에 구현 및 디버그가 쉽습니다. 
- 특정 Item을 추천하는 이유를 정당화하기 쉽고 Item 기반 방법의 해석 가능성이 두드러집니다. 
- 추천 리스트에 새로운 item과 user가 추가되어도 상대적으로 안정적입니다. 

[단점] 

- User 기반 방법의 시간, 속도, 메모리가 많이 필요합니다. 
- 희소성 때문에 제한된 범위가 있습니다. 
  - John의 Top-K 에만 관심이 있습니다. 
  - John과 비슷한 이웃중에서 아무도 해리포터를 평가하지 않으면, John의 해리포터에 대한 등급 예측을 제공할 수가 없습니다. 